# hadoop-streaming-data-pipeline
This project is adapted from Road to Data Engineer 2.0 course. It aimed to implement batch and streaming data pipeline using the hadoop ecosystem.

## DATA PIPELINE ARCHITECTURE:
![hadoop-pipeline](https://user-images.githubusercontent.com/48947748/161917432-7c47733b-d8ee-4193-9383-0368473a20c5.png)
  - Data Source <br /> The data source in the batch layer is from RDBMS (MySQL) and is ingested into HDFS with Sqoop. The second data source in the streaming layer is generated by shell script every 30 seconds in a log file and is ingested into HDFS and Hbase with Flume.
  - Batch Layer and Streaming Layer <br /> spark and spark-streaming are used to transform the data that store in HDFS in the batch layer and streaming layer, respectively, and then keep a table in Hive.
  - Workflow Scheduler <br /> using oozie to schedule tasks daily to insert joined data in batch layer and streaming layer to Hive.

## DATA MODEL
