# hadoop-data-pipeline
This project is adapted from Road to Data Engineer 2.0 course. It aimed to implement data pipeline using the hadoop ecosystem on cloudera.

## DATA PIPELINE ARCHITECTURE:
![161917432-7c47733b-d8ee-4193-9383-0368473a20c5](https://user-images.githubusercontent.com/48947748/173557874-717c486d-fc50-457a-a647-f703aaa69e65.png)<br />

The data pipeline was created on VMs in GCP and used service in Cloudera platform through docker.
  - Data Source <br /> The data source in the batch layer is from RDBMS (MySQL) and is ingested into HDFS with Sqoop. The second data source in the streaming layer is generated by shell script every 30 seconds in a log file and is ingested into HDFS and Hbase with Flume.
  - Batch Layer and Streaming Layer <br /> spark and spark-streaming are used to transform the data that store in HDFS in the batch layer and streaming layer, respectively, and then keep a table in Hive.
  - Workflow Scheduler <br /> using oozie to schedule tasks daily to insert joined data in batch layer and streaming layer to Hive.

## DATA MODEL:
![Untitled Diagram drawio](https://user-images.githubusercontent.com/48947748/161922660-8ffd6efc-79a8-4357-8fbd-affec2fb5c6f.png)

## SAMPLE DATA:

![customers_cln_ex](https://user-images.githubusercontent.com/48947748/161923370-68c17d10-0b00-4157-9caa-1b5e48dbfe40.jpg) <br />
<p align="center">customers_cln table</p>

![transactions_cln_ex](https://user-images.githubusercontent.com/48947748/161923385-2f9d31e3-3e63-4602-ae30-37796a2c70f7.jpg)<br />
<p align="center">transactions_cln table</p>

![loyalty_ex](https://user-images.githubusercontent.com/48947748/161923390-094bf80c-1930-41b6-8218-bffd3025a88b.jpg)<br />
<p align="center">loyalty table</p>
